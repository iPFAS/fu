{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5879c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, SelectFromModel\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_recall_curve, auc, mean_squared_error, \\\n",
    "    r2_score, mean_absolute_error,cohen_kappa_score,accuracy_score,f1_score,matthews_corrcoef,precision_score,recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "import multiprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "start = time.time()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def standardize(col):\n",
    "    return (col - np.mean(col)) / np.std(col)\n",
    "\n",
    "\n",
    "# the metrics for classification\n",
    "def statistical(y_true, y_pred, y_pro):\n",
    "    c_mat = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = list(c_mat.flatten())\n",
    "    se = tp / (tp + fn)\n",
    "    sp = tn / (tn + fp)\n",
    "    auc_prc = auc(precision_recall_curve(y_true, y_pro, pos_label=1)[1],\n",
    "                  precision_recall_curve(y_true, y_pro, pos_label=1)[0])\n",
    "    acc = (tp + tn) / (tn + fp + fn + tp)\n",
    "#     acc_skl = accuracy_score(y_true, y_pred)\n",
    "    auc_roc = roc_auc_score(y_true, y_pro)\n",
    "    recall = se\n",
    "#     recall_skl = recall_score(y_true, y_pred)\n",
    "    precision = tp / (tp + fp)\n",
    "#     precision_skl = precision_score(y_true, y_pred)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) # F1 = 2 * (precision * recall) / (precision + recall)\n",
    "#     f1_skl = f1_score(y_true, y_pred)\n",
    "    kappa = cohen_kappa_score(y_true,y_pred)\n",
    "    mcc = (tp * tn - fp * fn) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn) + 1e-8)\n",
    "#     mcc_skl = matthews_corrcoef(y_true,y_pred)\n",
    "    return tn,fp,fn,tp,se,sp,auc_prc,acc,auc_roc,recall,precision,f1,kappa,mcc\n",
    "\n",
    "def all_one_zeros(data):\n",
    "    if (len(np.unique(data)) == 2):\n",
    "        flag = False\n",
    "    else:\n",
    "        flag = True\n",
    "    return flag\n",
    "\n",
    "\n",
    "feature_selection = False\n",
    "tasks_dic = {'0-CF-2274-desc-split.csv': ['activity']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6daf58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '0-CF-2274-desc-split.csv'\n",
    "task_type = 'cla'  # 'reg' or 'cla'\n",
    "dataset_label = file_name.split('/')[-1].split('_')[0]\n",
    "tasks = tasks_dic[dataset_label]\n",
    "OPT_ITERS = 50\n",
    "repetitions = 50\n",
    "num_pools = 5\n",
    "unbalance = True\n",
    "patience = 100\n",
    "space_ = {'num_leaves': hp.choice('num_leaves', range(100, 300,10)),\n",
    "          'learning_rate': hp.uniform('learning_rate', 0.005, 0.3),\n",
    "          'max_depth': hp.choice('max_depth', range(3, 12)),\n",
    "          'n_estimators': hp.choice('n_estimators', [100,200,300, 400, 500, 1000]),\n",
    "          'min_child_samples': hp.choice('min_child_samples', range(0, 100,10)),\n",
    "          'max_bin': hp.choice('max_bin', range(300, 400,10))\n",
    "          }\n",
    "num_leaves_ls = range(100, 300,10)\n",
    "max_depth_ls = range(3, 12)\n",
    "n_estimators_ls = [100,200,300, 400, 500, 1000]\n",
    "min_child_samples_ls = range(0, 100,10)\n",
    "max_bin_ls = range(300, 400,10)\n",
    "dataset = pd.read_csv(file_name)\n",
    "pd_res = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5508043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_runing(subtask):\n",
    "    cols = [subtask]\n",
    "    cols.extend(dataset.columns[(len(tasks) + 1):])\n",
    "    sub_dataset = dataset[cols]\n",
    "\n",
    "    # detect the na in the subtask (y cloumn)\n",
    "    rm_index = sub_dataset[subtask][sub_dataset[subtask].isnull()].index\n",
    "    sub_dataset.drop(index=rm_index, inplace=True)\n",
    "\n",
    "    # remove the features with na\n",
    "    # *******************\n",
    "    # demension reduction\n",
    "    # *******************\n",
    "    # Removing features with low variance\n",
    "    # threshold = 0.05\n",
    "    data_fea_var = sub_dataset.iloc[:, 2:].var()\n",
    "    del_fea1 = list(data_fea_var[data_fea_var <= 0.05].index)\n",
    "    sub_dataset.drop(columns=del_fea1, inplace=True)\n",
    "\n",
    "    # pair correlations\n",
    "    # threshold = 0.95\n",
    "    data_fea_corr = sub_dataset.iloc[:, 2:].corr()\n",
    "    del_fea2_col = []\n",
    "    del_fea2_ind = []\n",
    "    length = data_fea_corr.shape[1]\n",
    "    for i in range(length):\n",
    "        for j in range(i + 1, length):\n",
    "            if abs(data_fea_corr.iloc[i, j]) >= 0.95:\n",
    "                del_fea2_col.append(data_fea_corr.columns[i])\n",
    "                del_fea2_ind.append(data_fea_corr.index[j])\n",
    "    sub_dataset.drop(columns=del_fea2_ind, inplace=True)\n",
    "\n",
    "    # standardize the features\n",
    "    cols_ = list(sub_dataset.columns)[2:]\n",
    "    sub_dataset[cols_] = sub_dataset[cols_].apply(standardize, axis=0)\n",
    "\n",
    "    # get the attentivefp data splits\n",
    "    data_tr = sub_dataset[sub_dataset['group'] == 'train']\n",
    "    data_va = sub_dataset[sub_dataset['group'] == 'valid']\n",
    "    data_te = sub_dataset[sub_dataset['group'] == 'test']\n",
    "\n",
    "    # prepare data for training\n",
    "    # training set\n",
    "    data_tr_y = data_tr[subtask].values.reshape(-1, 1)\n",
    "    data_tr_x = np.array(data_tr.iloc[:, 2:].values)\n",
    "\n",
    "    # validation set\n",
    "    data_va_y = data_va[subtask].values.reshape(-1, 1)\n",
    "    data_va_x = np.array(data_va.iloc[:, 2:].values)\n",
    "\n",
    "    # test set\n",
    "    data_te_y = data_te[subtask].values.reshape(-1, 1)\n",
    "    data_te_x = np.array(data_te.iloc[:, 2:].values)\n",
    "\n",
    "    if feature_selection:\n",
    "        # univariate feature selection\n",
    "        trans1 = SelectPercentile(f_classif, percentile=80)\n",
    "        trans1.fit(data_tr_x, data_tr_y)\n",
    "        data_tr_x = trans1.transform(data_tr_x)\n",
    "        data_va_x = trans1.transform(data_va_x)\n",
    "        data_te_x = trans1.transform(data_te_x)\n",
    "\n",
    "        # select from model\n",
    "        clf = XGBClassifier(n_jobs=12, random_state=1)\n",
    "        clf = clf.fit(data_tr_x, data_tr_y)\n",
    "        trans2 = SelectFromModel(clf, prefit=True)\n",
    "\n",
    "        data_tr_x = trans2.transform(data_tr_x)\n",
    "        data_va_x = trans2.transform(data_va_x)\n",
    "        data_te_x = trans2.transform(data_te_x)\n",
    "\n",
    "    num_fea = data_tr_x.shape[1]\n",
    "    print('the num of retained features for the ' + dataset_label + ' ' + subtask + ' is:', num_fea)\n",
    "\n",
    "    def hyper_opt(args):\n",
    "        model = LGBMClassifier(**args, n_jobs=6, random_state=1,\n",
    "                              is_unbalance = unbalance) if task_type == 'cla' else LGBMRegressor(**args, n_jobs=6,\n",
    "                                                                                                   random_state=1)\n",
    "\n",
    "        model.fit(data_tr_x, data_tr_y, eval_metric='auc' if task_type == 'cla' else 'rmse',\n",
    "                  eval_set=[(data_va_x, data_va_y)],\n",
    "                  early_stopping_rounds=patience, verbose=False)\n",
    "        val_preds = model.predict_proba(data_va_x) if task_type == 'cla' else \\\n",
    "            model.predict(data_va_x)\n",
    "        loss = 1 - roc_auc_score(data_va_y, val_preds[:, 1]) if task_type == 'cla' else np.sqrt(\n",
    "            mean_squared_error(data_va_y, val_preds))\n",
    "        return {'loss': loss, 'status': STATUS_OK}\n",
    "\n",
    "    # start hyper-parameters optimization\n",
    "    trials = Trials()\n",
    "    best_results = fmin(hyper_opt, space_, algo=tpe.suggest, max_evals=OPT_ITERS, trials=trials, show_progressbar=False)\n",
    "    print('the best hyper-parameters for ' + dataset_label + ' ' + subtask + ' are:  ', best_results)\n",
    "    best_model = LGBMClassifier(num_leaves=num_leaves_ls[best_results['num_leaves']],\n",
    "                                        learning_rate=best_results['learning_rate'],\n",
    "                                        max_depth=max_depth_ls[best_results['max_depth']],\n",
    "                                        n_estimators=n_estimators_ls[best_results['n_estimators']],\n",
    "                                        min_child_samples=min_child_samples_ls[best_results['min_child_samples']],\n",
    "                                        max_bin=max_bin_ls[best_results['max_bin']],\n",
    "                                        n_jobs=6, random_state=1, verbose=-1, is_unbalance = unbalance) \\\n",
    "        if task_type == 'cla' else LGBMRegressor(\n",
    "        num_leaves=num_leaves_ls[best_results['num_leaves']],\n",
    "        learning_rate=best_results['learning_rate'],\n",
    "        max_depth=max_depth_ls[best_results['max_depth']],\n",
    "        n_estimators=n_estimators_ls[best_results['n_estimators']],\n",
    "        min_child_samples=min_child_samples_ls[best_results['min_child_samples']],\n",
    "        max_bin=max_bin_ls[best_results['max_bin']],\n",
    "        n_jobs=6, random_state=1, verbose=-1)  \n",
    "    \n",
    "    best_model.fit(data_tr_x, data_tr_y, eval_metric='auc' if task_type == 'cla' else 'rmse',\n",
    "                   eval_set=[(data_va_x, data_va_y)],\n",
    "                   early_stopping_rounds=patience, verbose=False)\n",
    "    num_of_compounds = len(sub_dataset)\n",
    "\n",
    "    if task_type == 'cla':\n",
    "        # training set\n",
    "        tr_pred = best_model.predict_proba(data_tr_x)\n",
    "        tr_results = [dataset_label, subtask, 'tr', num_fea, num_of_compounds, data_tr_y[data_tr_y == 1].shape[0],\n",
    "                      data_tr_y[data_tr_y == 0].shape[0],\n",
    "                      data_tr_y[data_tr_y == 0].shape[0] / data_tr_y[data_tr_y == 1].shape[0],\n",
    "                      num_leaves_ls[best_results['num_leaves']],\n",
    "                      best_results['learning_rate'],\n",
    "                      max_depth_ls[best_results['max_depth']],\n",
    "                      n_estimators_ls[best_results['n_estimators']],\n",
    "                      min_child_samples_ls[best_results['min_child_samples']],\n",
    "                      max_bin_ls[best_results['max_bin']]]\n",
    "        tr_results.extend(statistical(data_tr_y, np.argmax(tr_pred, axis=1), tr_pred[:, 1]))\n",
    "        # validation set\n",
    "        va_pred = best_model.predict_proba(data_va_x)\n",
    "                      \n",
    "        va_results = [dataset_label, subtask, 'va', num_fea, num_of_compounds, data_va_y[data_va_y == 1].shape[0],\n",
    "                      data_va_y[data_va_y == 0].shape[0],\n",
    "                      data_va_y[data_va_y == 0].shape[0] / data_va_y[data_va_y == 1].shape[0],\n",
    "                      num_leaves_ls[best_results['num_leaves']],\n",
    "                      best_results['learning_rate'],\n",
    "                      max_depth_ls[best_results['max_depth']],\n",
    "                      n_estimators_ls[best_results['n_estimators']],\n",
    "                      min_child_samples_ls[best_results['min_child_samples']],\n",
    "                      max_bin_ls[best_results['max_bin']]]\n",
    "        va_results.extend(statistical(data_va_y, np.argmax(va_pred, axis=1), va_pred[:, 1]))\n",
    "\n",
    "        # test set\n",
    "        te_pred = best_model.predict_proba(data_te_x)\n",
    "        te_results = [dataset_label, subtask, 'te', num_fea, num_of_compounds, data_te_y[data_te_y == 1].shape[0],\n",
    "                      data_te_y[data_te_y == 0].shape[0],\n",
    "                      data_te_y[data_te_y == 0].shape[0] / data_te_y[data_te_y == 1].shape[0],\n",
    "                      num_leaves_ls[best_results['num_leaves']],\n",
    "                      best_results['learning_rate'],\n",
    "                      max_depth_ls[best_results['max_depth']],\n",
    "                      n_estimators_ls[best_results['n_estimators']],\n",
    "                      min_child_samples_ls[best_results['min_child_samples']],\n",
    "                      max_bin_ls[best_results['max_bin']]]\n",
    "        te_results.extend(statistical(data_te_y, np.argmax(te_pred, axis=1), te_pred[:, 1]))\n",
    "    else:\n",
    "        # training set\n",
    "        tr_pred = best_model.predict(data_tr_x)\n",
    "        tr_results = [dataset_label, subtask, 'tr', num_fea, num_of_compounds,\n",
    "                      num_leaves_ls[best_results['num_leaves']],\n",
    "                      best_results['learning_rate'],\n",
    "                      max_depth_ls[best_results['max_depth']],\n",
    "                      n_estimators_ls[best_results['n_estimators']],\n",
    "                      min_child_samples_ls[best_results['min_child_samples']],\n",
    "                      max_bin_ls[best_results['max_bin']],\n",
    "                      np.sqrt(mean_squared_error(data_tr_y, tr_pred)), r2_score(data_tr_y, tr_pred),\n",
    "                      mean_absolute_error(data_tr_y, tr_pred)]\n",
    "\n",
    "        # validation set\n",
    "        va_pred = best_model.predict(data_va_x)\n",
    "        va_results = [dataset_label, subtask, 'va', num_fea, num_of_compounds,\n",
    "                      num_leaves_ls[best_results['num_leaves']],\n",
    "                      best_results['learning_rate'],\n",
    "                      max_depth_ls[best_results['max_depth']],\n",
    "                      n_estimators_ls[best_results['n_estimators']],\n",
    "                      min_child_samples_ls[best_results['min_child_samples']],\n",
    "                      max_bin_ls[best_results['max_bin']],\n",
    "                      np.sqrt(mean_squared_error(data_va_y, va_pred)), r2_score(data_va_y, va_pred),\n",
    "                      mean_absolute_error(data_va_y, va_pred)]\n",
    "\n",
    "        # test set\n",
    "        te_pred = best_model.predict(data_te_x)\n",
    "        te_results = [dataset_label, subtask, 'te', num_fea, num_of_compounds,\n",
    "                      num_leaves_ls[best_results['num_leaves']],\n",
    "                      best_results['learning_rate'],\n",
    "                      max_depth_ls[best_results['max_depth']],\n",
    "                      n_estimators_ls[best_results['n_estimators']],\n",
    "                      min_child_samples_ls[best_results['min_child_samples']],\n",
    "                      max_bin_ls[best_results['max_bin']],\n",
    "                      np.sqrt(mean_squared_error(data_te_y, te_pred)), r2_score(data_te_y, te_pred),\n",
    "                      mean_absolute_error(data_te_y, te_pred)]\n",
    "    return tr_results, va_results, te_results\n",
    "\n",
    "\n",
    "pool = multiprocessing.Pool(num_pools)\n",
    "res = pool.starmap(hyper_runing, zip(tasks))\n",
    "pool.close()\n",
    "pool.join()\n",
    "for item in res:\n",
    "    for i in range(3):\n",
    "        pd_res.append(item[i])\n",
    "if task_type == 'cla':\n",
    "    best_hyper = pd.DataFrame(pd_res, columns=['dataset', 'subtask', 'set',\n",
    "                                               'num_of_retained_feature',\n",
    "                                               'num_of_compounds', 'postives',\n",
    "                                               'negtives', 'negtives/postives',\n",
    "                                               'num_leaves', \n",
    "                                               'learning_rate','max_depth','n_estimators','min_child_samples','max_bin',\n",
    "                                               'tn', 'fp', 'fn', 'tp', 'se', 'sp',\n",
    "                                               'auc_prc', 'acc', 'auc_roc','recall','precision','f1','kappa','mcc'])\n",
    "else:\n",
    "    best_hyper = pd.DataFrame(pd_res, columns=['dataset', 'subtask', 'set',\n",
    "                                               'num_leaves', \n",
    "                                               'learning_rate','max_depth','n_estimators','min_child_samples','max_bin',\n",
    "                                               'rmse', 'r2', 'mae'])\n",
    "best_hyper.to_csv('./stat_res/' + dataset_label + '_moe_pubsub_LGB_hyperopt_info.csv', index=0)\n",
    "\n",
    "if task_type == 'cla':\n",
    "    print('train', best_hyper[best_hyper['set'] == 'tr']['auc_roc'].mean(),\n",
    "          best_hyper[best_hyper['set'] == 'tr']['auc_prc'].mean())\n",
    "    print('valid', best_hyper[best_hyper['set'] == 'va']['auc_roc'].mean(),\n",
    "          best_hyper[best_hyper['set'] == 'va']['auc_prc'].mean())\n",
    "    print('test', best_hyper[best_hyper['set'] == 'te']['auc_roc'].mean(),\n",
    "          best_hyper[best_hyper['set'] == 'te']['auc_prc'].mean())\n",
    "else:\n",
    "    print('train', best_hyper[best_hyper['set'] == 'tr']['rmse'].mean(),\n",
    "          best_hyper[best_hyper['set'] == 'tr']['r2'].mean(), best_hyper[best_hyper['set'] == 'tr']['mae'].mean())\n",
    "    print('valid', best_hyper[best_hyper['set'] == 'va']['rmse'].mean(),\n",
    "          best_hyper[best_hyper['set'] == 'va']['r2'].mean(), best_hyper[best_hyper['set'] == 'va']['mae'].mean())\n",
    "    print('test', best_hyper[best_hyper['set'] == 'te']['rmse'].mean(),\n",
    "          best_hyper[best_hyper['set'] == 'te']['r2'].mean(), best_hyper[best_hyper['set'] == 'te']['mae'].mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e5b8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 repetitions based on thr best hypers\n",
    "dataset.drop(columns=['group'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "49892864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seed used in repetition 13 is 13\n",
      "random seed used in repetition 1 is 1\n",
      "random seed used in repetition 10 is 10\n",
      "random seed used in repetition 4 is 4\n",
      "random seed used in repetition 7 is 7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 5 is 5\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 14 is 14\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 8 is 8\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 11 is 11\n",
      "random seed used in repetition 2 is 2\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 15 is 15\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 6 is 6\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 9 is 9\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 16 is 16\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 19 is 19\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 3 is 3\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 12 is 12\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 17 is 17\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 22 is 22\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 20 is 20\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 25 is 25\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 18 is 18\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 28 is 28\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 21 is 21\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 23 is 23\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 31 is 31\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 26 is 26\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 34 is 34\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 29 is 29\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 24 is 24\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 27 is 27\n",
      "random seed used in repetition 32 is 32\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 35 is 35\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 30 is 30\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 37 is 37\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 33 is 33\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 40 is 40\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "random seed used in repetition 36 is 36\n",
      "random seed used in repetition 43 is 43\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 46 is 46\n",
      "random seed used in repetition 38 is 38\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 49 is 49\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 44 is 44\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 41 is 41\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 47 is 47\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 50 is 50\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 45 is 45\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 39 is 39\n",
      "random seed used in repetition 42 is 42\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "random seed used in repetition 48 is 48\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7344966859224016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7344966859224016\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "0-CF-2274-desc-split.csv_XGB: the mean auc_roc for the training set is 0.991 with std 0.015\n",
      "0-CF-2274-desc-split.csv_XGB: the mean auc_roc for the validation set is 0.878 with std 0.030\n",
      "0-CF-2274-desc-split.csv_XGB: the mean auc_roc for the test set is 0.864 with std 0.031\n",
      "the total elapsed time is: 2649.787378549576 S\n"
     ]
    }
   ],
   "source": [
    "pd_res = []\n",
    "def best_model_runing(split):\n",
    "    seed = split\n",
    "    if task_type == 'cla':\n",
    "        while True:\n",
    "            training_data, data_te = train_test_split(sub_dataset, test_size=0.1, random_state=seed)\n",
    "            # the training set was further splited into the training set and validation set\n",
    "            data_tr, data_va = train_test_split(training_data, test_size=0.1, random_state=seed)\n",
    "            if (all_one_zeros(data_tr[subtask]) or all_one_zeros(data_va[subtask]) or all_one_zeros(data_te[subtask])):\n",
    "                print(\n",
    "                    '\\ninvalid random seed {} due to one class presented in the {} splitted sets...'.format(seed,\n",
    "                                                                                                            subtask))\n",
    "                print('Changing to another random seed...\\n')\n",
    "                seed = np.random.randint(50, 999999)\n",
    "            else:\n",
    "                print('random seed used in repetition {} is {}'.format(split, seed))\n",
    "                break\n",
    "    else:\n",
    "        training_data, data_te = train_test_split(sub_dataset, test_size=0.1, random_state=seed)\n",
    "        # the training set was further splited into the training set and validation set\n",
    "        data_tr, data_va = train_test_split(training_data, test_size=0.1, random_state=seed)\n",
    "\n",
    "    # prepare data for training\n",
    "    # training set\n",
    "    data_tr_y = data_tr[subtask].values.reshape(-1, 1)\n",
    "    data_tr_x = np.array(data_tr.iloc[:, 1:].values)\n",
    "\n",
    "    # validation set\n",
    "    data_va_y = data_va[subtask].values.reshape(-1, 1)\n",
    "    data_va_x = np.array(data_va.iloc[:, 1:].values)\n",
    "\n",
    "    # test set\n",
    "    data_te_y = data_te[subtask].values.reshape(-1, 1)\n",
    "    data_te_x = np.array(data_te.iloc[:, 1:].values)\n",
    "\n",
    "    if feature_selection:\n",
    "        # univariate feature selection\n",
    "        trans1 = SelectPercentile(f_classif, percentile=80)\n",
    "        trans1.fit(data_tr_x, data_tr_y)\n",
    "        data_tr_x = trans1.transform(data_tr_x)\n",
    "        data_va_x = trans1.transform(data_va_x)\n",
    "        data_te_x = trans1.transform(data_te_x)\n",
    "\n",
    "        # select from model\n",
    "        clf = XGBClassifier(n_jobs=6, random_state=1)\n",
    "        clf = clf.fit(data_tr_x, data_tr_y)\n",
    "        trans2 = SelectFromModel(clf, prefit=True)\n",
    "\n",
    "        data_tr_x = trans2.transform(data_tr_x)\n",
    "        data_va_x = trans2.transform(data_va_x)\n",
    "        data_te_x = trans2.transform(data_te_x)\n",
    "\n",
    "    num_fea = data_tr_x.shape[1]\n",
    "    pos_weight = (len(sub_dataset) - sum(sub_dataset[subtask])) / sum(sub_dataset[subtask])\n",
    "#     model = LGBMClassifier(num_leaves=best_hyper[best_hyper.subtask == subtask].iloc[0,]['num_leaves'],\n",
    "#                           learning_rate=best_hyper[best_hyper.subtask == subtask].iloc[0,]['learning_rate'],\n",
    "#                           max_depth=best_hyper[best_hyper.subtask == subtask].iloc[0,]['max_depth'],\n",
    "#                           n_estimators=best_hyper[best_hyper.subtask == subtask].iloc[0,]['n_estimators'],\n",
    "#                           min_child_samples=best_hyper[best_hyper.subtask == subtask].iloc[0,]['min_child_samples'],\n",
    "#                           n_jobs=6, random_state=1,is_unbalance = unbalance) \\\n",
    "\n",
    "# Set the hyper-parameters based on previous experience .Use the above comment code if you need to tune hyper parameters\n",
    "    model =LGBMClassifier(\n",
    "                        boosting_type = 'gbdt',\n",
    "                        class_weight = None,\n",
    "                        colsample_bytree = 1.0,\n",
    "                        importance_type = 'split',\n",
    "                        learning_rate  = 0.1219922716918981,\n",
    "                        max_depth  = 15,\n",
    "                        min_child_samples  = 58,\n",
    "                        min_child_weight  = 0.001,\n",
    "                        min_split_gain  = 0.0,\n",
    "                        n_estimators  = 300,\n",
    "                        num_leaves  = 189,\n",
    "                        objective  = None,\n",
    "                        reg_alpha  = 8.031751803464846e-06,\n",
    "                        reg_lambda  = 0.0649255805574003,\n",
    "                        silent  = True,\n",
    "                        subsample  = 1.0,\n",
    "                        subsample_for_bin  = 200000,\n",
    "                        subsample_freq  = 0,\n",
    "                        bagging_fraction  = 1.0,\n",
    "                        bagging_freq  = 2,\n",
    "                        feature_fraction  = 0.7344966859224016,\n",
    "                          n_jobs=6, random_state=1,\n",
    "                        ) \\\n",
    "        if task_type == 'cla' else LGBMRegressor(\n",
    "        num_leaves=best_hyper[best_hyper.subtask == subtask].iloc[0,]['num_leaves'],\n",
    "        learning_rate=best_hyper[best_hyper.subtask == subtask].iloc[0,]['learning_rate'],\n",
    "        max_depth=best_hyper[best_hyper.subtask == subtask].iloc[0,]['max_depth'],\n",
    "        n_estimators=best_hyper[best_hyper.subtask == subtask].iloc[0,]['n_estimators'],\n",
    "        min_child_samples=best_hyper[best_hyper.subtask == subtask].iloc[0,]['min_child_samples'],\n",
    "        n_jobs=6, random_state=1, seed=1)\n",
    "\n",
    "    model.fit(data_tr_x, data_tr_y, eval_metric='auc' if task_type == 'cla' else 'rmse',\n",
    "              eval_set=[(data_va_x, data_va_y)],\n",
    "              early_stopping_rounds=patience, verbose=False)\n",
    "    num_of_compounds = sub_dataset.shape[0]\n",
    "    if task_type == 'cla':\n",
    "        # training set\n",
    "        tr_pred = model.predict_proba(data_tr_x)\n",
    "        tr_results = [split, dataset_label, subtask, 'tr', num_fea, num_of_compounds,\n",
    "                      data_tr_y[data_tr_y == 1].shape[0],\n",
    "                      data_tr_y[data_tr_y == 0].shape[0],\n",
    "                      data_tr_y[data_tr_y == 0].shape[0] / data_tr_y[data_tr_y == 1].shape[0]]\n",
    "        tr_results.extend(statistical(data_tr_y, np.argmax(tr_pred, axis=1), tr_pred[:, 1]))\n",
    "\n",
    "        # validation set\n",
    "        va_pred = model.predict_proba(data_va_x)\n",
    "        va_results = [split, dataset_label, subtask, 'va', num_fea, num_of_compounds,\n",
    "                      data_va_y[data_va_y == 1].shape[0],\n",
    "                      data_va_y[data_va_y == 0].shape[0],\n",
    "                      data_va_y[data_va_y == 0].shape[0] / data_va_y[data_va_y == 1].shape[0]]\n",
    "        va_results.extend(statistical(data_va_y, np.argmax(va_pred, axis=1), va_pred[:, 1]))\n",
    "\n",
    "        # test set\n",
    "        te_pred = model.predict_proba(data_te_x)\n",
    "        te_results = [split, dataset_label, subtask, 'te', num_fea, num_of_compounds,\n",
    "                      data_te_y[data_te_y == 1].shape[0],\n",
    "                      data_te_y[data_te_y == 0].shape[0],\n",
    "                      data_te_y[data_te_y == 0].shape[0] / data_te_y[data_te_y == 1].shape[0]]\n",
    "        te_results.extend(statistical(data_te_y, np.argmax(te_pred, axis=1), te_pred[:, 1]))\n",
    "    else:\n",
    "        # training set\n",
    "        tr_pred = model.predict(data_tr_x)\n",
    "        tr_results = [split, dataset_label, subtask, 'tr', num_fea, num_of_compounds,\n",
    "                      np.sqrt(mean_squared_error(data_tr_y, tr_pred)), r2_score(data_tr_y, tr_pred),\n",
    "                      mean_absolute_error(data_tr_y, tr_pred)]\n",
    "\n",
    "        # validation set\n",
    "        va_pred = model.predict(data_va_x)\n",
    "        va_results = [split, dataset_label, subtask, 'va', num_fea, num_of_compounds,\n",
    "                      np.sqrt(mean_squared_error(data_va_y, va_pred)), r2_score(data_va_y, va_pred),\n",
    "                      mean_absolute_error(data_va_y, va_pred)]\n",
    "\n",
    "        # test set\n",
    "        te_pred = model.predict(data_te_x)\n",
    "        te_results = [split, dataset_label, subtask, 'te', num_fea, num_of_compounds,\n",
    "                      np.sqrt(mean_squared_error(data_te_y, te_pred)), r2_score(data_te_y, te_pred),\n",
    "                      mean_absolute_error(data_te_y, te_pred)]\n",
    "    return tr_results, va_results, te_results\n",
    "\n",
    "\n",
    "for subtask in tasks:\n",
    "    cols = [subtask]\n",
    "    cols.extend(dataset.columns[(len(tasks) + 1):])\n",
    "    # cols.extend(dataset.columns[(617+1):])\n",
    "    sub_dataset = dataset[cols]\n",
    "\n",
    "    # detect the NA in the subtask (y cloumn)\n",
    "    rm_index = sub_dataset[subtask][sub_dataset[subtask].isnull()].index\n",
    "    sub_dataset.drop(index=rm_index, inplace=True)\n",
    "\n",
    "    # remove the features with na\n",
    "    if dataset_label != 'hiv':\n",
    "        sub_dataset = sub_dataset.dropna(axis=1)\n",
    "    else:\n",
    "        sub_dataset = sub_dataset.dropna(axis=0)\n",
    "\n",
    "    # *******************\n",
    "    # demension reduction\n",
    "    # *******************\n",
    "    # Removing features with low variance\n",
    "    # threshold = 0.05\n",
    "    data_fea_var = sub_dataset.iloc[:, 1:].var()\n",
    "    del_fea1 = list(data_fea_var[data_fea_var <= 0.05].index)\n",
    "    sub_dataset.drop(columns=del_fea1, inplace=True)\n",
    "\n",
    "    # pair correlations\n",
    "    # threshold = 0.95\n",
    "    data_fea_corr = sub_dataset.iloc[:, 1:].corr()\n",
    "    del_fea2_col = []\n",
    "    del_fea2_ind = []\n",
    "    length = data_fea_corr.shape[1]\n",
    "    for i in range(length):\n",
    "        for j in range(i + 1, length):\n",
    "            if abs(data_fea_corr.iloc[i, j]) >= 0.95:\n",
    "                del_fea2_col.append(data_fea_corr.columns[i])\n",
    "                del_fea2_ind.append(data_fea_corr.index[j])\n",
    "    sub_dataset.drop(columns=del_fea2_ind, inplace=True)\n",
    "\n",
    "    # standardize the features\n",
    "    cols_ = list(sub_dataset.columns)[1:]\n",
    "    sub_dataset[cols_] = sub_dataset[cols_].apply(standardize, axis=0)\n",
    "\n",
    "    # for split in range(1, splits+1):\n",
    "    pool = multiprocessing.Pool(num_pools)\n",
    "    res = pool.starmap(best_model_runing, zip(range(1, repetitions + 1)))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    for item in res:\n",
    "        for i in range(3):\n",
    "            pd_res.append(item[i])\n",
    "if task_type == 'cla':\n",
    "    stat_res = pd.DataFrame(pd_res, columns=['split', 'dataset', 'subtask', 'set',\n",
    "                                             'num_of_retained_feature',\n",
    "                                             'num_of_compounds', 'postives',\n",
    "                                             'negtives', 'negtives/postives',\n",
    "                                             'tn', 'fp', 'fn', 'tp', 'se', 'sp',\n",
    "                                             'auc_prc', 'acc', 'auc_roc','recall','precision','f1','kappa','mcc'])\n",
    "else:\n",
    "    stat_res = pd.DataFrame(pd_res, columns=['split', 'dataset', 'subtask', 'set',\n",
    "                                             'num_of_retained_feature',\n",
    "                                             'num_of_compounds', 'rmse', 'r2', 'mae'])\n",
    "stat_res.to_csv('./stat_res/' + dataset_label + '_xgb_statistical_results_split50_20220820.csv', index=0)\n",
    "# single tasks\n",
    "if len(tasks) == 1:\n",
    "    args = {'data_label': dataset_label, 'metric': 'auc_roc' if task_type == 'cla' else 'rmse', 'model': 'XGB'}\n",
    "    print('{}_{}: the mean {} for the training set is {:.3f} with std {:.3f}'.format(args['data_label'], args['model'],\n",
    "                                                                                     args['metric'], np.mean(\n",
    "            stat_res[stat_res['set'] == 'tr'][args['metric']]), np.std(\n",
    "            stat_res[stat_res['set'] == 'tr'][args['metric']])))\n",
    "    print(\n",
    "        '{}_{}: the mean {} for the validation set is {:.3f} with std {:.3f}'.format(args['data_label'], args['model'],\n",
    "                                                                                     args['metric'], np.mean(\n",
    "                stat_res[stat_res['set'] == 'va'][args['metric']]), np.std(\n",
    "                stat_res[stat_res['set'] == 'va'][args['metric']])))\n",
    "    print('{}_{}: the mean {} for the test set is {:.3f} with std {:.3f}'.format(args['data_label'], args['model'],\n",
    "                                                                                 args['metric'], np.mean(\n",
    "            stat_res[stat_res['set'] == 'te'][args['metric']]), np.std(\n",
    "            stat_res[stat_res['set'] == 'te'][args['metric']])))\n",
    "# multi-tasks\n",
    "else:\n",
    "    args = {'data_label': dataset_label, 'metric': 'auc_roc' if dataset_label != 'muv' else 'auc_prc', 'model': 'LGB'}\n",
    "    tr_acc = np.zeros(repetitions)\n",
    "    va_acc = np.zeros(repetitions)\n",
    "    te_acc = np.zeros(repetitions)\n",
    "    for subtask in tasks:\n",
    "        tr = stat_res[stat_res['set'] == 'tr']\n",
    "        tr_acc = tr_acc + tr[tr['subtask'] == subtask][args['metric']].values\n",
    "\n",
    "        va = stat_res[stat_res['set'] == 'va']\n",
    "        va_acc = va_acc + va[va['subtask'] == subtask][args['metric']].values\n",
    "\n",
    "        te = stat_res[stat_res['set'] == 'te']\n",
    "        te_acc = te_acc + te[te['subtask'] == subtask][args['metric']].values\n",
    "    tr_acc = tr_acc / len(tasks)\n",
    "    va_acc = va_acc / len(tasks)\n",
    "    te_acc = te_acc / len(tasks)\n",
    "    print('{}_{}: the mean {} for the training set is {:.3f} with std {:.3f}'.format(args['data_label'], args['model'],\n",
    "                                                                                     args['metric'], np.mean(tr_acc),\n",
    "                                                                                     np.std(tr_acc)))\n",
    "    print(\n",
    "        '{}_{}: the mean {} for the validation set is {:.3f} with std {:.3f}'.format(args['data_label'], args['model'],\n",
    "                                                                                     args['metric'], np.mean(va_acc),\n",
    "                                                                                     np.std(va_acc)))\n",
    "    print('{}_{}: the mean {} for the test set is {:.3f} with std {:.3f}'.format(args['data_label'], args['model'],\n",
    "                                                                                 args['metric'], np.mean(te_acc),\n",
    "                                                                                 np.std(te_acc)))\n",
    "end = time.time()  # get the end time\n",
    "print('the total elapsed time is:', (end - start), 'S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "50bc4eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the elapsed time is: 0.7360520495971044 H\n"
     ]
    }
   ],
   "source": [
    "# acc auc_roc recall precision f1 kappa mcc\n",
    "acc_str = 'acc of training set is {:.3f}{:.3f}, validation set is {:.3f}{:.3f}, test set is {:.3f}{:.3f}'.format(\n",
    "                np.mean(stat_res[stat_res['set'] == 'tr']['acc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'tr']['acc']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'va']['acc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'va']['acc']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['acc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'te']['acc']),\n",
    ")\n",
    "auc_str = 'auc_roc of training set is {:.3f}{:.3f}, validation set is {:.3f}{:.3f}, test set is {:.3f}{:.3f}'.format(\n",
    "                np.mean(stat_res[stat_res['set'] == 'tr']['auc_roc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'tr']['auc_roc']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'va']['auc_roc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'va']['auc_roc']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['auc_roc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'te']['auc_roc']),\n",
    ")\n",
    "recall_str = 'recall of training set is {:.3f}{:.3f}, validation set is {:.3f}{:.3f}, test set is {:.3f}{:.3f}'.format(\n",
    "                np.mean(stat_res[stat_res['set'] == 'tr']['recall']), \n",
    "                np.std(stat_res[stat_res['set'] == 'tr']['recall']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'va']['recall']), \n",
    "                np.std(stat_res[stat_res['set'] == 'va']['recall']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['recall']), \n",
    "                np.std(stat_res[stat_res['set'] == 'te']['recall']),\n",
    ")\n",
    "precision_str = 'precision of training set is {:.3f}{:.3f}, validation set is {:.3f}{:.3f}, test set is {:.3f}{:.3f}'.format(\n",
    "                np.mean(stat_res[stat_res['set'] == 'tr']['precision']), \n",
    "                np.std(stat_res[stat_res['set'] == 'tr']['precision']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'va']['precision']), \n",
    "                np.std(stat_res[stat_res['set'] == 'va']['precision']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['precision']), \n",
    "                np.std(stat_res[stat_res['set'] == 'te']['precision']),\n",
    ")\n",
    "f1_str = 'f1 of training set is {:.3f}{:.3f}, validation set is {:.3f}{:.3f}, test set is {:.3f}{:.3f}'.format(\n",
    "                np.mean(stat_res[stat_res['set'] == 'tr']['f1']), \n",
    "                np.std(stat_res[stat_res['set'] == 'tr']['f1']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'va']['f1']), \n",
    "                np.std(stat_res[stat_res['set'] == 'va']['f1']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['f1']), \n",
    "                np.std(stat_res[stat_res['set'] == 'te']['f1']),\n",
    ")\n",
    "kappa_str = 'kappa of training set is {:.3f}{:.3f}, validation set is {:.3f}{:.3f}, test set is {:.3f}{:.3f}'.format(\n",
    "                np.mean(stat_res[stat_res['set'] == 'tr']['kappa']), \n",
    "                np.std(stat_res[stat_res['set'] == 'tr']['kappa']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'va']['kappa']), \n",
    "                np.std(stat_res[stat_res['set'] == 'va']['kappa']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['kappa']), \n",
    "                np.std(stat_res[stat_res['set'] == 'te']['kappa']),\n",
    ")\n",
    "mcc_str = 'mcc of training set is {:.3f}{:.3f}, validation set is {:.3f}{:.3f}, test set is {:.3f}{:.3f}'.format(\n",
    "                np.mean(stat_res[stat_res['set'] == 'tr']['mcc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'tr']['mcc']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'va']['mcc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'va']['mcc']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['mcc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'te']['mcc']),\n",
    ")\n",
    "print('the elapsed time is:', (end - start)/3600, 'H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "bfb4f708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc of training set is 0.9640.036, validation set is 0.8740.023, test set is 0.8670.023\n",
      "auc_roc of training set is 0.9910.015, validation set is 0.8780.030, test set is 0.8640.031\n",
      "recall of training set is 0.8210.193, validation set is 0.4400.089, test set is 0.4290.102\n",
      "precision of training set is 0.9760.020, validation set is 0.7570.099, test set is 0.7390.112\n",
      "f1 of training set is 0.8770.159, validation set is 0.5490.089, test set is 0.5320.106\n",
      "kappa of training set is 0.8590.170, validation set is 0.4830.091, test set is 0.4620.105\n",
      "mcc of training set is 0.8690.149, validation set is 0.5100.082, test set is 0.4890.097\n"
     ]
    }
   ],
   "source": [
    "print(acc_str)\n",
    "print(auc_str)\n",
    "print(recall_str)\n",
    "print(precision_str)\n",
    "print(f1_str)\n",
    "print(kappa_str)\n",
    "print(mcc_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "ee4ebafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output_lgb_20220906.txt', 'w') as f:\n",
    "    f.write(acc_str+'\\n')\n",
    "    f.write(auc_str+'\\n')\n",
    "    f.write(recall_str+'\\n')\n",
    "    f.write(precision_str+'\\n')\n",
    "    f.write(f1_str+'\\n')\n",
    "    f.write(kappa_str+'\\n')\n",
    "    f.write(mcc_str+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf9e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-dgl]",
   "language": "python",
   "name": "conda-env-anaconda3-dgl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
